
#include <stdio.h>  
#include <stdlib.h> 
#include <mpi.h>    


#define N 8  // 8 строк
#define M 10 // 10 столбцов

/*
 * Вспомогательная функция для заполнения матрицы тестовыми данными.
 * Принимает указатель на 1D-массив (который представляет 2D-матрицу)
 */
void initialize_matrix(double *matrix, int rows, int cols) {
    // Проходим по каждой строке
    for (int i = 0; i < rows; i++) {
        // Проходим по каждому столбцу
        for (int j = 0; j < cols; j++) {
            // matrix[i][j] в 1D-массиве = matrix[i * cols + j]
            matrix[i * cols + j] = i + j; // Заполняем данными i+j
        }
    }
}

// Главная функция программы
int main(int argc, char** argv) {
    
    // Инициализация MPI
    MPI_Init(&argc, &argv);

    // Переменные для хранения ID процесса и общего числа процессов
    int world_size, world_rank;

    // Получаем общее число процессов (np), запущенных mpirun
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    
    // Получаем "ранг" (уникальный ID от 0 до np-1) этого процесса
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // Проверка входных данных
    // Наша программа для простоты (из-за MPI_Scatter) требует,
    // чтобы число строк (N) делилось нацело на число процессов.
    if (N % world_size != 0) {
        
        // Только "главный" процесс (ранг 0) печатает ошибку,
        // чтобы избежать вывода N одинаковых ошибок.
        if (world_rank == 0) {
            fprintf(stderr, "Ошибка: N (%d) должно делиться нацело на число процессов (%d)\n", N, world_size);
        }
        
        MPI_Finalize(); // Завершаем MPI
        return 1;       // Выходим с кодом ошибки
    }

    //Создание и инициализация данных (только на процессе 0)

    // Указатель на всю матрицу.
    // У rank 0 он будет указывать на данные, у всех остальных = NULL.
    double *global_matrix = NULL; 

    if (world_rank == 0) {
        printf("Запуск на %d процессах. Размер матрицы: %d x %d\n", world_size, N, M);
        
        // Выделяем память под всю матрицу N*M
        global_matrix = (double*)malloc(N * M * sizeof(double));
        
        // Заполняем ее данными
        initialize_matrix(global_matrix, N, M);
    }

    // Расчет размеров "кусков" для каждого процесса
    
    // (N / world_size) -> сколько строк достанется каждому
    // Пример: 8 строк / 4 процесса = 2 строки на процесс
    int rows_per_proc = N / world_size;
    
    // (rows_per_proc * M) -> сколько *элементов* в куске
    // Пример: 2 строки * 10 столбцов = 20 элементов на процесс
    int elements_per_proc = rows_per_proc * M;

    // Выделение локальной памяти
    
    // каждый  процесс (включая 0) выделяет память
    // только для своего  маленького куска
    double *local_chunk = (double*)malloc(elements_per_proc * sizeof(double));

    // Распределение работы (Scatter)
    
    // MPI_Scatter - коллективная операция.
    // Процесс 0 "нарезает" global_matrix и "раздает" куски всем.
    MPI_Scatter(
        global_matrix,      // Указатель на все данные (только у rank 0)
        elements_per_proc,  // *Сколько* элементов отправить *каждому*
        MPI_DOUBLE,         // Тип данных
        local_chunk,        // *Куда* принять свою часть (у *каждого* процесса)
        elements_per_proc,  // *Сколько* элементов *я* принимаю
        MPI_DOUBLE,         // Тип данных
        0,                  // Ранг процесса-отправителя (root)
        MPI_COMM_WORLD      // Коммуникатор
    );
    // После этого у rank 0 в local_chunk лежат строки 0-1,
    // у rank 1 - строки 2-3, у rank 2 - строки 4-5, и т.д.

    // Локальные вычисления (Параллельная часть)
    
    // каждый процесс независимо считает сумму *только* своего куска
    double local_sum = 0.0;
    for (int i = 0; i < elements_per_proc; i++) {
        local_sum += local_chunk[i];
    }
    // Теперь у rank 0 есть сумма строк 0-1, у rank 1 - сумма строк 2-3...

    // Переменная для хранения итоговой суммы (будет заполнена только у rank 0)
    double global_sum = 0.0;

    // Сбор результатов (Reduce)
    
    // MPI_Reduce - коллективная операция.
    // Все процессы "отправляют" свой local_sum процессу 0.
    // Процесс 0 "собирает" их и выполняет операцию MPI_SUM.
    MPI_Reduce(
        &local_sum,         // Указатель на мои данные для отправки
        &global_sum,        // Указатель на итог (заполняется только у rank 0)
        1,                  // Количество элементов (мы отправляем 1 число)
        MPI_DOUBLE,         // Тип данных
        MPI_SUM,            // Операция (сложить все полученные local_sum)
        0,                  // Ранг процесса-получателя (root)
        MPI_COMM_WORLD      // Коммуникатор
    );

    // Вывод и очистка
    
    // Только rank 0 выводит результат и выполняет проверку
    if (world_rank == 0) {
        printf("Общая параллельная сумма: %f\n", global_sum);

        // --- Проверка (считаем то же самое, но в 1 поток) ---
        double serial_sum = 0.0;
        for (int i = 0; i < N * M; i++) {
            serial_sum += global_matrix[i];
        }
        printf("Общая последовательная сумма (для проверки): %f\n", serial_sum);

        // Только rank 0 освобождает память из-под *всей* матрицы
        free(global_matrix);
    }

    // каждый  процесс освобождает память из-под своего куска
    free(local_chunk);

    // Завершение MPI. Обязательный вызов.
    MPI_Finalize();

    return 0; // Успешный выход
}
